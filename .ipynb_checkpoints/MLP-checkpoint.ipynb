{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff0e578-8036-416a-8c5b-456e7baeb1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'targets'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_2756\\4064139860.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  t = torch.load('part_one_dataset/train_data/1_train_data.tar.pth')\n"
     ]
    }
   ],
   "source": [
    "        import torch\n",
    "        t = torch.load('part_one_dataset/train_data/1_train_data.tar.pth')\n",
    "        print(t.keys()) # it will print data and targets\n",
    "        data, targets = t['data'], t['targets'] # both numpy.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba1eac54-d48a-4254-8205-b9503910c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached torchvision-0.20.1-cp312-cp312-win_amd64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.5.1 in c:\\users\\kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (2.5.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1->torchvision) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1->torchvision) (70.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\kumar\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch==2.5.1->torchvision) (2.1.5)\n",
      "Using cached torchvision-0.20.1-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\kumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\kumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\kumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22b89057-49ee-4467-b3f4-bb269189397c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kumar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\kumar/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:11<00:00, 4.24MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# Load a pre-trained model, e.g., ResNet18\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Remove the last fully connected layer to get features instead of predictions\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "model.eval()  # Set to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3376915b-a4ca-4941-a777-efab52d32f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert numpy array to PIL Image\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224 as required by most pre-trained models\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42d2741-250b-486e-8d77-cf90dbb49d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_24684\\3057522370.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  t = torch.load('part_one_dataset/train_data/1_train_data.tar.pth')\n"
     ]
    }
   ],
   "source": [
    "# Load the data and targets from D1\n",
    "t = torch.load('part_one_dataset/train_data/1_train_data.tar.pth')\n",
    "data, targets = t['data'], t['targets']  # numpy.ndarray\n",
    "\n",
    "# Convert numpy array to list of tensors after transformation\n",
    "data_tensors = [transform(img).unsqueeze(0) for img in data]  # unsqueeze to add batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffdb9298-bf26-456b-9829-31dba10cb256",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "\n",
    "# Loop through each image tensor to extract features\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "    for img_tensor in data_tensors:\n",
    "        feature = model(img_tensor)  # Pass through model\n",
    "        feature = feature.squeeze().numpy()  # Remove extra dimensions and convert to numpy\n",
    "        features.append(feature)\n",
    "\n",
    "# Convert features list to a numpy array for easier manipulation\n",
    "features = np.array(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c79b63b-e397-423d-b4bd-cf38393a70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features and labels to a .npy file\n",
    "np.save(\"D1_features.npy\", features)\n",
    "np.save(\"D1_labels.npy\", targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b78560bb-5a57-4d23-b460-dce8e6b67427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f1.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load labeled data from D1\n",
    "D1_features = np.load(\"D1_features.npy\")\n",
    "D1_labels = np.load(\"D1_labels.npy\")\n",
    "\n",
    "# Initialize and train the MLP classifier on D1\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, activation='relu', solver='adam')\n",
    "classifier.fit(D1_features, D1_labels)\n",
    "\n",
    "# Save the initial classifier (f1)\n",
    "joblib.dump(classifier, \"f1.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1c2a897-3554-4380-8a29-81ee6c41e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_24684\\473036878.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  eval_data = torch.load(\"part_one_dataset/eval_data/1_eval_data.tar.pth\")  # Replace with the actual path\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load evaluation data (assuming it has the same structure as the training data file)\n",
    "eval_data = torch.load(\"part_one_dataset/eval_data/1_eval_data.tar.pth\")  # Replace with the actual path\n",
    "eval_images, eval_labels = eval_data['data'], eval_data['targets']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cac2cd86-0a5e-4b64-b759-5c16b6f445f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_features = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation for efficiency\n",
    "    for img in eval_images:\n",
    "        img_tensor = transform(img).unsqueeze(0)  # Add batch dimension\n",
    "        feature = model(img_tensor)  # Extract features\n",
    "        feature = feature.squeeze().numpy()  # Convert to numpy array\n",
    "        eval_features.append(feature)\n",
    "\n",
    "# Convert the list to a numpy array for easy handling\n",
    "eval_features = np.array(eval_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08918553-b8a8-4a6d-8f58-0d4b9adf1ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"eval_features.npy\", eval_features)\n",
    "np.save(\"eval_labels.npy\", eval_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b7de165-0cbb-4813-9089-34f366e56a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model f1 on evaluation data: 0.8488\n",
      "Accuracy trend on evaluation data across models f1 to f10: [0.8488]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load saved evaluation features and labels\n",
    "eval_features = np.load(\"eval_features.npy\")\n",
    "eval_labels = np.load(\"eval_labels.npy\")\n",
    "\n",
    "# List to store accuracy for each model\n",
    "accuracies = []\n",
    "\n",
    "# Iterate over each model f1 to f10\n",
    "for i in range(1, 2):\n",
    "    # Load the model fi\n",
    "    classifier = joblib.load(f\"f{i}.pkl\")\n",
    "    \n",
    "    # Predict on eval_features\n",
    "    eval_predictions = classifier.predict(eval_features)\n",
    "    \n",
    "    # Calculate accuracy of fi on eval data\n",
    "    accuracy = accuracy_score(eval_labels, eval_predictions)\n",
    "    accuracies.append(accuracy)\n",
    "    \n",
    "    # Print accuracy for this model\n",
    "    print(f\"Accuracy of model f{i} on evaluation data: {accuracy:.4f}\")\n",
    "\n",
    "# Optional: Print the entire accuracy trend for better visualization\n",
    "print(\"Accuracy trend on evaluation data across models f1 to f10:\", accuracies)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
